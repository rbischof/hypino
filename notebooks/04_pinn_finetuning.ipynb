{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f212de7",
   "metadata": {},
   "source": [
    "# PINN Initialization and Finetuning\n",
    "\n",
    "This notebook demonstrates **how to initialize and fine-tune Physics-Informed Neural Networks (PINNs)** using the [HyPINO multi-physics neural operator](https://arxiv.org/abs/2509.05117).\n",
    "\n",
    "HyPINO maps a given PDE specification — defined by its coefficients, source term, and boundary conditions — to a **set of pretrained network weights** that already approximate the corresponding solution field.  \n",
    "These weights can then be used to initialize a PINN for **task-specific adaptation or fine-tuning**.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Generate an initial PINN** with weights $\\theta^\\star = \\Phi(L, f, g, h)$ predicted by HyPINO,  \n",
    "   where $(L, f, g, h)$ denote the PDE operator, source term, and boundary data.  \n",
    "\n",
    "2. **Evaluate the initialized model** to obtain the zero-shot solution $u_{\\theta^\\star}(x)$.  \n",
    "\n",
    "3. **Fine-tune the PINN** by minimizing the residual loss $\\mathcal{L}_{\\text{PINN}} = \\lambda_R \\mathcal{L}_R + \\lambda_D \\mathcal{L}_D + \\lambda_N \\mathcal{L}_N$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fa0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.models import HyPINO\n",
    "from src.data.utils import to_tensor\n",
    "from src.data.utils import plot_grids, encode_pde_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = f'cuda:{torch.cuda.current_device()}'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20158470",
   "metadata": {},
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyPINO.load_from_safetensors('../models/hypino.safetensors').to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d192859",
   "metadata": {},
   "source": [
    "## Load example PDE\n",
    "See the `02_inference.ipynb` notebook for examples on how to load or create other PDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfdce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_path = '../assets/wave/arrays'\n",
    "\n",
    "# inputs\n",
    "dirichlet_mask = np.load(os.path.join(inputs_path, 'dirichlet_mask.npy'))\n",
    "dirichlet_conditions = np.load(os.path.join(inputs_path, 'dirichlet_conditions.npy'))\n",
    "neumann_mask = np.load(os.path.join(inputs_path, 'neumann_mask.npy'))\n",
    "neumann_conditions = np.load(os.path.join(inputs_path, 'neumann_conditions.npy'))\n",
    "source_function = np.load(os.path.join(inputs_path, 'source_function.npy'))\n",
    "\n",
    "# if necessary, load domain mask\n",
    "domain_mask = np.load(os.path.join(inputs_path, 'domain_mask.npy'))\n",
    "\n",
    "# if necessary, load neumann normals for computing correct boundary losses\n",
    "if os.path.exists(os.path.join(inputs_path, 'neumann_normals.npy')):\n",
    "        neumann_normals = np.load(os.path.join(inputs_path, 'neumann_normals.npy'))\n",
    "else:\n",
    "        neumann_normals = np.zeros((2, 224, 224))\n",
    "\n",
    "# if available, load reference solution\n",
    "reference_solution = np.load(os.path.join(inputs_path, 'reference_solution.npy'))\n",
    "\n",
    "plot_grids([dirichlet_mask, dirichlet_conditions, neumann_mask, neumann_conditions, \n",
    "            neumann_normals[0], neumann_normals[1], source_function, domain_mask, reference_solution], \n",
    "           titles=['Dirichlet mask', 'Dirichlet boundary conditions', 'Neumann mask', \n",
    "                   'Neumann boundary conditions', 'Neumann normals x', 'Neumann normals y', \n",
    "                   'Source function', 'Domain mask', 'Reference solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5131058",
   "metadata": {},
   "source": [
    "Create the grid-based inputs to HyPINO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_inputs = np.stack([dirichlet_mask, neumann_mask,\n",
    "                   dirichlet_conditions, neumann_conditions,\n",
    "                   source_function], axis=0)\n",
    "mat_inputs_tensor = to_tensor(mat_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981adec",
   "metadata": {},
   "source": [
    "Create the vector of coefficients for HyPINO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4766877",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_operator = '0.5 * uyy - 2 * uxx'\n",
    "pde_coeffs = encode_pde_str(diff_operator)\n",
    "pde_coeffs_tensor = to_tensor([c for c in pde_coeffs.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676447fb",
   "metadata": {},
   "source": [
    "Prepare inputs as dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "pde = {\n",
    "    'pde_coeffs': pde_coeffs_tensor.to(device),\n",
    "    'mat_inputs': mat_inputs_tensor.to(device),\n",
    "    'neu_normals': to_tensor(neumann_normals).to(device),\n",
    "    'pde_str': diff_operator,\n",
    "    'domain_mask': to_tensor(domain_mask).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a54a1",
   "metadata": {},
   "source": [
    "## Generate PINN\n",
    "Generate the target PINN for the given PDE. Optionally, create an ensemble of PINNs, where the ensemble is iteratively expanded by generating and adding a PINN that corrects the residual of the ensemble in the previous iteration. See the `03_iterative_refinement.ibynb` notebook for more examples and explanations.\n",
    "\n",
    "Set `num_iter=0` to skip iterative refinement and use just the first predicted PINN (ensemble of 1 expert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_ensemble = model.iterative_refinement(pde, num_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76b17c",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "We provide the `finetuning` method for convenience. It takes the PDE and the PINN (or ensemble) and trains it by computing the residual and errors on the Dirichlet and Neumann boundaries. It internally creates a grid of collocation points, of which a random subset of size `num_collocation_points` are used in each iteration to compute the loss (ideally set this value as high as possible before running into OOM errors). \n",
    "\n",
    "Training runs for `num_adam_iterations` iterations with the Adam optimizer, before switching to LBFGS for `num_lbfgs_iterations` iterations. \n",
    "\n",
    "Further, optional arguments are `loss_weights={'F':0.1,'D':10,'N':5}` to set the weights per loss (residual `F`, Dirichlet `D`, Neumann `N`), `eval_every` to set the interval of evaluations, and `plot_path` to optionally save the plots. \n",
    "\n",
    "Note that this method creates a grid of collocation points and uses the Dirichlet and Neumann masks from above to identify which of them lie on a boundary. This approach generally results in far fewer points on the boundaries compared to the interior of the domain. This imbalance can be a limitation, as PINN training often requires a higher density of boundary points to ensure the boundary conditions are well-approximated. Therefore, we provide the `boundary_oversample` argument that can be used to balance the number of collocation points on the boundaries and inside the domain, either by passing the string `'balanced'` or a float between 0 and 1 denoting the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c781f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.utils import finetuning\n",
    "\n",
    "hist = finetuning(pde=pde, net=pinn_ensemble, num_adam_iterations=4500, num_lbfgs_iterations=500, \n",
    "                  num_collocation_points=4096, eval_every=10, loss_weights={'F': 0.01, 'D': 10, 'N': 5},\n",
    "                  boundary_oversample='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pinn_ensemble, 'finetuned_wave_ensemble.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
